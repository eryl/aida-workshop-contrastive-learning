{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the softmax \"layer\" do?\n",
    "\n",
    "When we train a model on classification we use a linear output layer followed by a softmax activation to scale the outputs of the linear layer. In this notebook we'll take a closer look at what this layer actually learn when using a softmax output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset\n",
    "We start by setting up the data. We will use MNIST as is the tradition in deep learning for imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandChannels(nn.Module):\n",
    "    def __init__(self, num_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "    \n",
    "    def forward(self, img):\n",
    "        # Count channels from back, so we can use it both for single examples and batches\n",
    "        *leading_dims, n_channels, height, width = img.shape\n",
    "        if n_channels != self.num_channels:\n",
    "            repeats = [1 for d in leading_dims] + [self.num_channels, 1, 1]\n",
    "            new_img = img.repeat(*repeats)\n",
    "        else:\n",
    "            new_img = img\n",
    "        return new_img\n",
    "        \n",
    "    \n",
    "\n",
    "dataset_transforms = transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True), ExpandChannels(3)])\n",
    "train_dataset = MNIST(root='../datasets', download=True, transform=dataset_transforms)\n",
    "test_dataset = MNIST(root='../datasets', train=False, download=True, transform=dataset_transforms)\n",
    "num_classes = len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at some examples. We take the first unique images of each class\n",
    "unique_targets, target_indices = np.unique(train_dataset.targets, return_index=True)\n",
    "fig, axises = plt.subplots(3,4)\n",
    "for ax in axises.flatten():\n",
    "    ax.set_axis_off()\n",
    "for target_index, ax in zip(target_indices, axises.flatten()):\n",
    "    tensor_image, label = train_dataset[target_index]\n",
    "    # torchvision images are channel-first, we transpose \n",
    "    # it so channel is last and convert it to a numpy array\n",
    "    numpy_image = tensor_image.permute(1, 2, 0).numpy()\n",
    "    ax.imshow(numpy_image)\n",
    "fig.suptitle(\"MNIST Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, dev_indices = train_test_split(np.arange(len(train_dataset)),  test_size=0.1, random_state=42, stratify=train_dataset.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_targets = zip(*[train_dataset[i] for i in train_indices])\n",
    "dev_images, dev_targets = zip(*[train_dataset[i] for i in dev_indices])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.stack(train_images)\n",
    "train_targets = torch.tensor(train_targets)\n",
    "dev_images = torch.stack(dev_images)\n",
    "dev_targets = torch.tensor(dev_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_split_dataset = TensorDataset(train_images, train_targets)\n",
    "dev_split_dataset = TensorDataset(dev_images, dev_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The visualization subset\n",
    "\n",
    "To illustrate what the model learns, we'll select a subset of the training data to use to calculate features. The reason for selecting a subset of the training data is for us to understand what the model is actually doing while it's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well make a small subselection of the training samples\n",
    "n = 20\n",
    "visualization_sample_index, _ = train_test_split(np.arange(len(train_images)), shuffle=True, stratify=train_targets, train_size=n*num_classes)\n",
    "visualization_images = torch.stack([train_images[i] for i in visualization_sample_index])\n",
    "visualization_targets = torch.stack([train_targets[i] for i in visualization_sample_index])\n",
    "visualization_dataset = TensorDataset(visualization_images, visualization_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "We'll create a simple resnet-based model and focus on the layers just before applying the softmax function. We replace the pre-defined fully connected layers with a single two-part bottleneck. We select the dimensionality of this bottleneck such that we can visualize it (e.g. 2D or 3D). In the first case we will make things neat by forcing the model to use unit-norm vectors for the bottleneck representations as well as the output weights. This means that the only thing the network can change to change the prediction output is the angle between the bottleneck representations and the output weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): SphericalPredictionHead()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "model = resnet18(weights=None)\n",
    "\n",
    "# Determine the number of input features to the output layer:\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# We will create an \"prediction head\" which will actually be two linear layers with a bottlneck inbetween. \n",
    "# The linear layers will be bottlenecked through a 2D feature space so that they live in a geometric space \n",
    "# we can visualize. We make this into a class because we want to easily manipulate the intermediate results\n",
    "\n",
    "class SphericalPredictionHead(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension, bottleneck_dimension=2, rng=None):\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.bottleneck_dimension = bottleneck_dimension\n",
    "        \n",
    "        # Since we actually only want linear transformations, which we're going to normalize before \n",
    "        # using, it's easier to just create the matrices explicitly. Note that we will multiply \n",
    "        # these matrices from the right since we assume that the first axis of the input is the \n",
    "        # batch dimension.\n",
    "        self.input_transform = nn.Parameter(torch.tensor(rng.normal(size=(input_dimension, bottleneck_dimension)), dtype=torch.float32))\n",
    "        self.output_transform = nn.Parameter(torch.tensor(rng.normal(size=(bottleneck_dimension, output_dimension)), dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_reduced = torch.matmul(x, self.input_transform)\n",
    "        x_normalized = normalize(x_reduced, dim=-1) # By using -1, we're saying that the normalization should be along the innermost axis.\n",
    "        output_transform_normalized = normalize(self.output_transform, dim=0)\n",
    "        # We're now taking the matrix multiplication between the normalized input vectors (all lying on the unit sphere) with the \n",
    "        # normalized \"softmax\" weights, also lying on the unit sphere. This is exactly the same as calculating the cosine of \n",
    "        # the angle between the vectors\n",
    "        cosines = torch.matmul(x_normalized, output_transform_normalized)\n",
    "        return cosines\n",
    "\n",
    "rng = np.random.default_rng(1729)\n",
    "prediction_head = SphericalPredictionHead(num_ftrs, num_classes, bottleneck_dimension=2, rng=rng)\n",
    "model.fc = prediction_head\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the feature representations\n",
    "\n",
    "What we're really interested in is getting the representations of our images just before we send them to the classification layer. This is how the neural network has learnt to represent the images to minimize the categorical cross entropy. We also want the normalized weights so that we can visualize them. The torchvision package has neat utilities for doing this. We can use the function `get_graph_node_names`to trace the compute graph of the model it can create a tree of nodes which are being called. This returns two lists of node names: one for the model in _training_ mode and one in _evaluation_ mode. In this case we're going to use the model in evaluation mode to get the representations, so the nodes we are interested in are in the second of the two returned lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'conv1',\n",
       " 'bn1',\n",
       " 'relu',\n",
       " 'maxpool',\n",
       " 'layer1.0.conv1',\n",
       " 'layer1.0.bn1',\n",
       " 'layer1.0.relu',\n",
       " 'layer1.0.conv2',\n",
       " 'layer1.0.bn2',\n",
       " 'layer1.0.add',\n",
       " 'layer1.0.relu_1',\n",
       " 'layer1.1.conv1',\n",
       " 'layer1.1.bn1',\n",
       " 'layer1.1.relu',\n",
       " 'layer1.1.conv2',\n",
       " 'layer1.1.bn2',\n",
       " 'layer1.1.add',\n",
       " 'layer1.1.relu_1',\n",
       " 'layer2.0.conv1',\n",
       " 'layer2.0.bn1',\n",
       " 'layer2.0.relu',\n",
       " 'layer2.0.conv2',\n",
       " 'layer2.0.bn2',\n",
       " 'layer2.0.downsample.0',\n",
       " 'layer2.0.downsample.1',\n",
       " 'layer2.0.add',\n",
       " 'layer2.0.relu_1',\n",
       " 'layer2.1.conv1',\n",
       " 'layer2.1.bn1',\n",
       " 'layer2.1.relu',\n",
       " 'layer2.1.conv2',\n",
       " 'layer2.1.bn2',\n",
       " 'layer2.1.add',\n",
       " 'layer2.1.relu_1',\n",
       " 'layer3.0.conv1',\n",
       " 'layer3.0.bn1',\n",
       " 'layer3.0.relu',\n",
       " 'layer3.0.conv2',\n",
       " 'layer3.0.bn2',\n",
       " 'layer3.0.downsample.0',\n",
       " 'layer3.0.downsample.1',\n",
       " 'layer3.0.add',\n",
       " 'layer3.0.relu_1',\n",
       " 'layer3.1.conv1',\n",
       " 'layer3.1.bn1',\n",
       " 'layer3.1.relu',\n",
       " 'layer3.1.conv2',\n",
       " 'layer3.1.bn2',\n",
       " 'layer3.1.add',\n",
       " 'layer3.1.relu_1',\n",
       " 'layer4.0.conv1',\n",
       " 'layer4.0.bn1',\n",
       " 'layer4.0.relu',\n",
       " 'layer4.0.conv2',\n",
       " 'layer4.0.bn2',\n",
       " 'layer4.0.downsample.0',\n",
       " 'layer4.0.downsample.1',\n",
       " 'layer4.0.add',\n",
       " 'layer4.0.relu_1',\n",
       " 'layer4.1.conv1',\n",
       " 'layer4.1.bn1',\n",
       " 'layer4.1.relu',\n",
       " 'layer4.1.conv2',\n",
       " 'layer4.1.bn2',\n",
       " 'layer4.1.add',\n",
       " 'layer4.1.relu_1',\n",
       " 'avgpool',\n",
       " 'flatten',\n",
       " 'fc.fc_input_transform',\n",
       " 'fc.matmul',\n",
       " 'fc.normalize',\n",
       " 'fc.fc_output_transform',\n",
       " 'fc.normalize_1',\n",
       " 'fc.matmul_1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "# Inspect the forward graph of our model to get the correct feature node\n",
    "train_nodes, eval_nodes = get_graph_node_names(model)\n",
    "eval_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that there are a couple of nodes whos names start with `fc.`, you might recognize them from our SphericalPredictionHead. The first `fc.normalize` is the one which has been applied to the bottlenecked features and is the first one we're after. The second normalization node, `fc.normalize_1`, is the result of normalizing the \"softmax\" weight vectors, which we also want.\n",
    "\n",
    "We can use  the convinience function `create_feature_extractor` to get a wrapper of the model which returns the values of the desired nodes. As an input we give it the names of the nodes we are interested in and what name we would like these values to have in the output of the feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(model, return_nodes={'fc.normalize': 'normalized_input_features', 'fc.normalize_1': 'normalized_softmax_weights'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this out on a part of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_input_features': tensor([[-0.7101, -0.7041]], device='cuda:0'),\n",
       " 'normalized_softmax_weights': tensor([[ 0.4241, -0.8653, -0.7037,  0.6079, -0.0956, -0.2707,  0.9309, -0.9995,\n",
       "           0.3497,  0.8962],\n",
       "         [-0.9056,  0.5012, -0.7105,  0.7940, -0.9954,  0.9627, -0.3653,  0.0320,\n",
       "           0.9369,  0.4436]], device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    x_0, y_0 = train_split_dataset[0]\n",
    "    features = feature_extractor(x_0.to(device).unsqueeze(0))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the feature extractor returns a dictionary where the keys are the target names we gave in the `return_nodes` argument to `create_feature_extractor`. The values are calculated results at the different compute nodes which we will store while we are training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "We'll now train the network for a couple of epochs. The important part is that we will pause the training with a regular interval to evaluate what the features for our selected visualization subset is, as well as what the softmax weight vectors for the output layer are. We'll then visualize these results after the training is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c08af6f019f48b8943fdd842c8c1cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1252333fea49adadf4d3af545272d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train batch:   0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9115d56a0083425f92729e6bfca6e55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dev batch:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "dev_dataloader = DataLoader(dev_split_dataset, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "visualization_dataloader = DataLoader(visualization_dataset, batch_size=batch_size, drop_last=False, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 5\n",
    "sample_step = 20\n",
    "n_iterations = 0\n",
    "visualization_representations = {i: [] for i in range(epochs)}\n",
    "visualization_weights = {i: None for i in range(epochs)}\n",
    "visualization_targets = None\n",
    "\n",
    "epoch_progress_bar = trange(epochs, desc=\"Epoch\")\n",
    "train_progress_bar = trange(len(train_dataloader), desc=\"Train batch\")\n",
    "dev_progress_bar = trange(len(dev_dataloader), desc=\"Dev batch\")\n",
    "\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_progress_bar.reset()\n",
    "    dev_progress_bar.reset()    \n",
    "    for train_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = train_batch\n",
    "        cosine_predictions = model(inputs.to(device))\n",
    "        loss = loss_fn(cosine_predictions, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if n_iterations % sample_step == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                viz_features = []\n",
    "                viz_targets = []\n",
    "                normalized_softmax_weights = None\n",
    "                for viz_batch in visualization_dataloader:\n",
    "                    inputs, targets = viz_batch\n",
    "                    features = feature_extractor(inputs.to(device))\n",
    "                    viz_features.append(features['normalized_input_features'].cpu())\n",
    "                    if normalized_softmax_weights is None:\n",
    "                        normalized_softmax_weights = features['normalized_softmax_weights'].cpu().numpy()\n",
    "                    viz_targets.append(targets.cpu())\n",
    "                concatenated_viz_features = torch.concat(viz_features).numpy()\n",
    "                concatenated_viz_targets = torch.concat(viz_targets).numpy()\n",
    "                if visualization_targets is None:\n",
    "                    visualization_targets = concatenated_viz_targets\n",
    "                visualization_representations[n_iterations//sample_step] = concatenated_viz_features\n",
    "                visualization_weights[n_iterations//sample_step] = normalized_softmax_weights\n",
    "            model.train()\n",
    "        n_iterations += 1\n",
    "        train_progress_bar.update()\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_dev_loss = 0\n",
    "        n_dev_samples = 0\n",
    "        for dev_batch in dev_dataloader:\n",
    "            inputs, targets = dev_batch\n",
    "            cosine_predictions = model(inputs.to(device))\n",
    "            loss = loss_fn(cosine_predictions, targets.to(device))\n",
    "            n = len(inputs)\n",
    "            batch_loss = loss.item() * n\n",
    "            total_dev_loss += batch_loss\n",
    "            n_dev_samples += n\n",
    "            dev_progress_bar.update()\n",
    "        #print(\"Dev loss: \", total_dev_loss/n_dev_samples)\n",
    "    epoch_progress_bar.update()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the features of the training data\n",
    "We want to inspect where the input features wound up in input space. To make the visualization meaningful we create a very small subset of the training data (20 samples per class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342ed7da18ad4657bbc8a6cf0f65cbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='epoch', max=843), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(epoch)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as colors\n",
    "#from matplotlib.animation import FuncAnimation\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "initial_representations = visualization_representations[0]\n",
    "initial_weights = visualization_weights[0]\n",
    "\n",
    "cmap = plt.get_cmap('tab10')\n",
    "cNorm  = colors.Normalize(vmin=min(train_targets), vmax=max(train_targets))\n",
    "\n",
    "class_index_to_value = {i: v for v,i in train_dataset.class_to_idx.items()}\n",
    "class_index_to_color = {i: cmap(cNorm(i)) for i in class_index_to_value.keys()}\n",
    "\n",
    "x_lim = (-1.1, 1.1)\n",
    "y_lim = (-1.1, 1.1)\n",
    "\n",
    "def update_plot(epoch):\n",
    "    figure = plt.figure(figsize=(6,6))\n",
    "    representations = visualization_representations[epoch]\n",
    "    plt.scatter(representations[:,0], representations[:,1], c=[class_index_to_color[x] for x in visualization_targets])\n",
    "    epoch_weights = visualization_weights[epoch]\n",
    "    for i, (x,y) in enumerate(epoch_weights.T):\n",
    "        label = class_index_to_value[i]\n",
    "        color = class_index_to_color[i]\n",
    "        plt.plot([0, x], [0,y], label=label, color=color)\n",
    "    plt.xlim(x_lim)\n",
    "    plt.ylim(y_lim)\n",
    "    \n",
    "\n",
    "interact(update_plot, epoch=widgets.IntSlider(min=0, max=max(visualization_representations.keys()), step=1, value=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what are the softmax vectors?\n",
    "As you can see in the example, each individual element of the output layer (the logit per class) is associated with a vector. A neural network learns to solve the classification problem by mapping each input element in the training data as close (in terms of angle) as possible to its class-vector. In many ways this is kind of like K-means clustering in the sense that we're learning a centroid or prototype vector per class. This shouldn't come as too much of a surprise if we remember the close relationship between the **dot product** and angles. The dot product of two vectors is the same as the cosine of the angle between them and scaled by the product of their magnitude:\n",
    "$$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = \\lvert \\mathbf{u} \\rvert \\lvert \\mathbf{v} \\rvert \\cos \\alpha\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the angle between the vectors. We can rewrite this to:\n",
    "$$\n",
    "\\cos \\alpha = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\lvert \\mathbf{u} \\rvert \\lvert \\mathbf{v} \\rvert} = \\frac{\\mathbf{u}}{\\lvert \\mathbf{u} \\rvert} \\cdot \\frac{\\mathbf{v}}{ \\lvert \\mathbf{v} \\rvert} \n",
    "$$\n",
    "\n",
    "In other words, the cosine of the angle between two vectors is the same as the dot product between the _normalized_ vectors which is **exactly** what we implemented in the _spherical_ prediction head. The fundamental way in which most of our neural networks solve their problems is by _minimizing angles between vectors_. This ofcourse only holds for neural networks which use the dot product as its main operation, but that holds for all neural networks we typically use today.\n",
    "\n",
    "We could think of our model as containing two main blocks: an _encoder_ and a _decoder_. The _encoder_ maps the inputs $x$ to the unit sphere: \n",
    "$$\n",
    "\\mathbf{z} = f_{enc}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "The decoder then maps the input vector to a categorical distribution:\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = f_{dec}(\\mathbf{z})\n",
    "$$\n",
    "\n",
    "The decoder is what drives the learning of the upstreams neural network. In our case, the decoder is simply a matrix multiplication;\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = W_s \\mathbf{z} = \\begin{bmatrix}\\mathbf{w}_{1} \\\\ \\mathbf{w}_{2} \\\\ \\vdots \\\\ \\mathbf{w}_{c} \\end{bmatrix} \\mathbf{z} = \\begin{bmatrix}\\mathbf{w}_{1} \\cdot \\mathbf{z} \\\\ \\mathbf{w}_{2} \\cdot \\mathbf{z} \\\\ \\vdots \\\\ \\mathbf{w}_{c} \\cdot \\mathbf{z} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The $w_i$ here are our softmax vectors, and the result of our output is actually the _unscaled_ cosine of the angle between the transformed input and our softmax vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try changing the code above to not train the softmax layer. You can do this by changing the input to the optimizer in the training cell:\n",
    "\n",
    "```python\n",
    "parameters = list(model.parameters())\n",
    "parameters = parameters[:-1] # Skip the last parameters, which we know is the output layer\n",
    "optimizer = torch.optim.AdamW(parameters, lr=1e-4)\n",
    "```\n",
    "\n",
    "We simply don't give the output layer parameters to the optimization algorithm which means they will keep whatever value we initialize them to. Another change we might want to do is to actually change the initialization of the prediction head to something like this:\n",
    "```python\n",
    "    def __init__(self, input_dimension, output_dimension, bottleneck_dimension=2, rng=None):\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.bottleneck_dimension = bottleneck_dimension\n",
    "        \n",
    "        # Since we actually only want linear transformations, which we're going to normalize before \n",
    "        # using, it's easier to just create the matrices explicitly. Note that we will multiply \n",
    "        # these matrices from the right since we assume that the first axis of the input is the \n",
    "        # batch dimension.\n",
    "        self.input_transform = nn.Parameter(torch.tensor(rng.normal(size=(input_dimension, 2)), dtype=torch.float32))\n",
    "        output_angles = np.linspace(0, 2*np.pi, output_dimension)\n",
    "        x = np.cos(output_angles)\n",
    "        y = np.sin(output_angles)\n",
    "        output_weights = np.stack((x,y))\n",
    "        self.output_transform = nn.Parameter(torch.tensor(output_weights), dtype=torch.float32)\n",
    "```\n",
    "This **hardcodes** a bottleneck dimension of 2, we chose to distribute the softmax vectors evenly along the unit circle. With other bottleneck dimensions, we'd have to do something more complicated. With _very_ high bottleneck dimension, we don't need to bother with manually spreading out the vectors. In high dimensional spaces random vectors will very likely be \"far away\" from each other in terms of angle.\n",
    "\n",
    "You can inspect the weight vectors with this snippet:\n",
    "```python\n",
    "for x,y in model.fc.output_transform.detach().cpu().numpy().T:\n",
    "    plt.plot([0,x], [0,y])\n",
    "```\n",
    "\n",
    "**Try training the model with these changes, what happens?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without normalizing the features and softmax weights\n",
    "In the example above, we forced the network to only use the angle between the vectors to solve the classification problem, which lended itself nicely to a visualization and makes the connection to the cosine _exact_. We'll now look at how the network behaves if we're instead letting it train without this constraint. Note that this is a departure from our main goal: understanding how we can implement contrastive learning, but we'd be leaving an important question hanging if we don't try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "\n",
    "model = resnet18(weights=None)\n",
    "\n",
    "# Determine the number of input features to the output layer:\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# We will create an \"prediction head\" which will actually be two linear layers with a bottlneck inbetween. \n",
    "# The linear layers will be bottlenecked through a 2D feature space so that they live in a geometric space \n",
    "# we can visualize. We make this into a class because we want to easily manipulate the intermediate results\n",
    "\n",
    "class BottleneckPredictionHead(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension, bottleneck_dimension=2, rng=None):\n",
    "        super().__init__()\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "        self.bottleneck_dimension = bottleneck_dimension\n",
    "        \n",
    "        # Since we actually only want linear transformations, which we're going to normalize before \n",
    "        # using, it's easier to just create the matrices explicitly. Note that we will multiply \n",
    "        # these matrices from the right since we assume that the first axis of the input is the \n",
    "        # batch dimension.\n",
    "        self.input_transform = nn.Parameter(torch.tensor(rng.normal(size=(input_dimension, bottleneck_dimension)), dtype=torch.float32))\n",
    "        self.output_transform = nn.Parameter(torch.tensor(rng.normal(size=(bottleneck_dimension, output_dimension)), dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_reduced = torch.matmul(x, self.input_transform)\n",
    "        logits = torch.matmul(x_reduced, self.output_transform)\n",
    "        return logits\n",
    "\n",
    "rng = np.random.default_rng(1729)\n",
    "prediction_head = BottleneckPredictionHead(num_ftrs, num_classes, bottleneck_dimension=2, rng=rng)\n",
    "model.fc = prediction_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'conv1',\n",
       " 'bn1',\n",
       " 'relu',\n",
       " 'maxpool',\n",
       " 'layer1.0.conv1',\n",
       " 'layer1.0.bn1',\n",
       " 'layer1.0.relu',\n",
       " 'layer1.0.conv2',\n",
       " 'layer1.0.bn2',\n",
       " 'layer1.0.add',\n",
       " 'layer1.0.relu_1',\n",
       " 'layer1.1.conv1',\n",
       " 'layer1.1.bn1',\n",
       " 'layer1.1.relu',\n",
       " 'layer1.1.conv2',\n",
       " 'layer1.1.bn2',\n",
       " 'layer1.1.add',\n",
       " 'layer1.1.relu_1',\n",
       " 'layer2.0.conv1',\n",
       " 'layer2.0.bn1',\n",
       " 'layer2.0.relu',\n",
       " 'layer2.0.conv2',\n",
       " 'layer2.0.bn2',\n",
       " 'layer2.0.downsample.0',\n",
       " 'layer2.0.downsample.1',\n",
       " 'layer2.0.add',\n",
       " 'layer2.0.relu_1',\n",
       " 'layer2.1.conv1',\n",
       " 'layer2.1.bn1',\n",
       " 'layer2.1.relu',\n",
       " 'layer2.1.conv2',\n",
       " 'layer2.1.bn2',\n",
       " 'layer2.1.add',\n",
       " 'layer2.1.relu_1',\n",
       " 'layer3.0.conv1',\n",
       " 'layer3.0.bn1',\n",
       " 'layer3.0.relu',\n",
       " 'layer3.0.conv2',\n",
       " 'layer3.0.bn2',\n",
       " 'layer3.0.downsample.0',\n",
       " 'layer3.0.downsample.1',\n",
       " 'layer3.0.add',\n",
       " 'layer3.0.relu_1',\n",
       " 'layer3.1.conv1',\n",
       " 'layer3.1.bn1',\n",
       " 'layer3.1.relu',\n",
       " 'layer3.1.conv2',\n",
       " 'layer3.1.bn2',\n",
       " 'layer3.1.add',\n",
       " 'layer3.1.relu_1',\n",
       " 'layer4.0.conv1',\n",
       " 'layer4.0.bn1',\n",
       " 'layer4.0.relu',\n",
       " 'layer4.0.conv2',\n",
       " 'layer4.0.bn2',\n",
       " 'layer4.0.downsample.0',\n",
       " 'layer4.0.downsample.1',\n",
       " 'layer4.0.add',\n",
       " 'layer4.0.relu_1',\n",
       " 'layer4.1.conv1',\n",
       " 'layer4.1.bn1',\n",
       " 'layer4.1.relu',\n",
       " 'layer4.1.conv2',\n",
       " 'layer4.1.bn2',\n",
       " 'layer4.1.add',\n",
       " 'layer4.1.relu_1',\n",
       " 'avgpool',\n",
       " 'flatten',\n",
       " 'fc.fc_input_transform',\n",
       " 'fc.matmul',\n",
       " 'fc.fc_output_transform',\n",
       " 'fc.matmul_1']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "# Inspect the forward graph of our model to get the correct feature node\n",
    "train_nodes, eval_nodes = get_graph_node_names(model)\n",
    "eval_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = create_feature_extractor(model, return_nodes={'fc.matmul': 'bottleneck_features', 'fc.fc_output_transform': 'softmax_weights'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30414b1825be4e01a3cf74c17dbbd5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805889b3a1a5467db6fec6bed04a0f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train batch:   0%|          | 0/844 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a95a5178e5406fb4edc7fae1fcf4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dev batch:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_split_dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "dev_dataloader = DataLoader(dev_split_dataset, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "visualization_dataloader = DataLoader(visualization_dataset, batch_size=batch_size, drop_last=False, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "sample_step = 20\n",
    "n_iterations = 0\n",
    "visualization_representations = {i: [] for i in range(epochs)}\n",
    "visualization_weights = {i: None for i in range(epochs)}\n",
    "visualization_targets = None\n",
    "\n",
    "epoch_progress_bar = trange(epochs, desc=\"Epoch\")\n",
    "train_progress_bar = trange(len(train_dataloader), desc=\"Train batch\")\n",
    "dev_progress_bar = trange(len(dev_dataloader), desc=\"Dev batch\")\n",
    "\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_progress_bar.reset()\n",
    "    dev_progress_bar.reset()    \n",
    "    for train_batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = train_batch\n",
    "        cosine_predictions = model(inputs.to(device))\n",
    "        loss = loss_fn(cosine_predictions, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if n_iterations % sample_step == 0:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                viz_features = []\n",
    "                viz_targets = []\n",
    "                normalized_softmax_weights = None\n",
    "                for viz_batch in visualization_dataloader:\n",
    "                    inputs, targets = viz_batch\n",
    "                    features = feature_extractor(inputs.to(device))\n",
    "                    viz_features.append(features['bottleneck_features'].cpu())\n",
    "                    if normalized_softmax_weights is None:\n",
    "                        normalized_softmax_weights = features['softmax_weights'].cpu().numpy()\n",
    "                    viz_targets.append(targets.cpu())\n",
    "                concatenated_viz_features = torch.concat(viz_features).numpy()\n",
    "                concatenated_viz_targets = torch.concat(viz_targets).numpy()\n",
    "                if visualization_targets is None:\n",
    "                    visualization_targets = concatenated_viz_targets\n",
    "                visualization_representations[n_iterations//sample_step] = concatenated_viz_features\n",
    "                visualization_weights[n_iterations//sample_step] = normalized_softmax_weights\n",
    "            model.train()\n",
    "        n_iterations += 1\n",
    "        train_progress_bar.update()\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_dev_loss = 0\n",
    "        n_dev_samples = 0\n",
    "        for dev_batch in dev_dataloader:\n",
    "            inputs, targets = dev_batch\n",
    "            cosine_predictions = model(inputs.to(device))\n",
    "            loss = loss_fn(cosine_predictions, targets.to(device))\n",
    "            n = len(inputs)\n",
    "            batch_loss = loss.item() * n\n",
    "            total_dev_loss += batch_loss\n",
    "            n_dev_samples += n\n",
    "            dev_progress_bar.update()\n",
    "        #print(\"Dev loss: \", total_dev_loss/n_dev_samples)\n",
    "    epoch_progress_bar.update()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_weights = visualization_weights[0]\n",
    "initial_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9d8f948b5147968385994f6037929a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='epoch', max=843), FloatSlider(value=5.0, description='we…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.update_plot(epoch, weights_scale=5.0)>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as colors\n",
    "#from matplotlib.animation import FuncAnimation\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "initial_representations = visualization_representations[0]\n",
    "initial_weights = visualization_weights[0]\n",
    "\n",
    "cmap = plt.get_cmap('tab10')\n",
    "cNorm  = colors.Normalize(vmin=min(train_targets), vmax=max(train_targets))\n",
    "\n",
    "class_index_to_value = {i: v for v,i in train_dataset.class_to_idx.items()}\n",
    "class_index_to_color = {i: cmap(cNorm(i)) for i in class_index_to_value.keys()}\n",
    "\n",
    "all_representations = np.concatenate(list(visualization_representations.values()))\n",
    "x_lim_points = (all_representations[:,0].min(), all_representations[:,0].max())\n",
    "y_lim_points = (all_representations[:,1].min(), all_representations[:,1].max())\n",
    "\n",
    "all_weights = np.concatenate(list(visualization_weights.values()),axis=1)\n",
    "x_lim_lines = (all_weights[0,:].min(), all_weights[0,:].max())\n",
    "y_lim_lines = (all_weights[1,:].min(), all_weights[1,:].max())\n",
    "\n",
    "def update_plot(epoch, weights_scale=5.):\n",
    "    figure = plt.figure(figsize=(6,6))\n",
    "    representations = visualization_representations[epoch]\n",
    "    plt.scatter(representations[:,0], representations[:,1], c=[class_index_to_color[x] for x in visualization_targets])\n",
    "    epoch_weights = visualization_weights[epoch]\n",
    "    for i, (x,y) in enumerate(epoch_weights.T):\n",
    "        label = class_index_to_value[i]\n",
    "        color = class_index_to_color[i]\n",
    "        plt.plot([0, weights_scale*x], [0,weights_scale*y], label=label, color=color)\n",
    "    plt.xlim(x_lim_points)\n",
    "    plt.ylim(y_lim_points)\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "\n",
    "interact(update_plot, epoch=widgets.IntSlider(min=0, max=max(visualization_representations.keys()), step=1, value=0), weights_scale=widgets.FloatSlider(min=0, max=400., step=0.1, value=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's much harder to see due to the very different scales, you can see that the class vectors still strive to align _radially_ with the softmax vectors. The problem is that the initial vectors have very different magnitudes so the transformed inputs need to be mapped to magnitudes which compensates for this. One important take-away from this is that _Euclidean distance_ directly on the intermediate representations are not the best measure of what the neural network tries to do. Instead, you should look at the \"cosine similarity\" between vectors, or if you need a proper distance metric use the angular distance or Euclidean distance on the _normalized_ vectors which from a distance ranking to neighbours is equivalent with cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "In this notebook we've focused on the role our class vectors in the output layer (just before softmax normalization) plays in driving the learning of the neural network. What we could see is that the fundamental similarity metric the neural networks use is the angle between vectors, due to the fact that we're mainly training the network to minimize angles between weight vectors. While we've only looked at the output layer, a similar observation could be made for all layers of the network. To get a unit at a particular layer to activate highly (disregarding the bias), the input vector to that unit should be as colinear as possible the the weight vector of that unit.\n",
    "\n",
    "\n",
    "Another point we could make here is that where to divide the network into the encoder and decoder part is up to us and what representations we are interested in. As we could see here, the representations at the next-to-last layer will be directly optimized to be as close as possible to our prototype vectors. Given enough capacity, the encoder network would have mapped all the training points to be exactly colinear to the prototype vectors. We would essentially have 10 (in the case of MNIST) very tightly packed clusters. This means that if we want to use the encoder neural network in _transfer learning_, these representation would be _overfit_ to the MNIST classification problem. They would likely be poor representations for other data such as general handwritten glyphs. So as a common piece of advice, if you are doing _transfer learning_, you probably don't want to take the representations just before the softmax weights.\n",
    "\n",
    "In the next notebook we'll take this observation, that the output layer is a set of _prototype_ vectors, one for each class, but we will do away with this notion and instead use the data examples themselves as the prototype vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "constrastive_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
